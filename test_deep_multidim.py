"""
Author         : AI Assistant
Date           : 2024-01-XX
Description    : æµ‹è¯•æ·±åº¦ç½‘ç»œåœ¨å¤šç»´æ•°æ®ä¸Šçš„è¡¨ç°
                è¾“å…¥æ ¼å¼: (batch_size, 6, len, dim)
                å…¶ä¸­: 6=å˜æ¢æ•°é‡, len=400(æ—¶é—´åºåˆ—é•¿åº¦), dim=æ•°æ®ç»´åº¦

å®éªŒè®¾ç½®:
- åˆ†ç±»æ•°: 3, 5
- ç»´åº¦: 1, 5, 8
- ç½‘ç»œ: æ·±åº¦CNN
"""

from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow as tf
import tensorflow_docs.modeling as tfdoc_model
import tensorflow_docs.plots as tfdoc_plot
from sklearn.model_selection import train_test_split
from datetime import datetime
import pandas as pd
import sys

# æ·»åŠ AutoCPDåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from autocpd.neuralnetwork import compile_and_fit
from autocpd.utils import DataSetGen, Transform2D2TR
from autocpd.high_dim_utils import HighDimDataSetGen, get_preset_config
from tensorflow.keras import layers, models


def create_multidim_deep_network(input_shape, n_classes, n_filter=16, dropout_rate=0.3, 
                                n_resblock=3, model_name="multidim_deep_nn"):
    """
    åˆ›å»ºé€‚ç”¨äº(6, len, dim)è¾“å…¥æ ¼å¼çš„æ·±åº¦ç½‘ç»œ
    
    Parameters:
    -----------
    input_shape : tuple
        è¾“å…¥å½¢çŠ¶ (6, len, dim)
    n_classes : int
        åˆ†ç±»æ•°é‡
    n_filter : int
        å·ç§¯æ»¤æ³¢å™¨æ•°é‡
    dropout_rate : float
        Dropoutç‡
    n_resblock : int
        æ®‹å·®å—æ•°é‡
    model_name : str
        æ¨¡å‹åç§°
    """
    n_trans, length, dim = input_shape
    
    # è¾“å…¥å±‚
    input_layer = layers.Input(shape=input_shape, name="Input")
    
    # åˆå§‹å·ç§¯å±‚
    x = layers.Conv2D(n_filter, (2, 2), padding="same")(input_layer)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    x = layers.MaxPooling2D((2, 2))(x)
    
    # æ®‹å·®å—
    for i in range(n_resblock):
        residual = x
        
        # ç¬¬ä¸€ä¸ªå·ç§¯
        x = layers.Conv2D(n_filter, (3, 3), padding="same")(x)
        x = layers.BatchNormalization()(x)
        x = layers.ReLU()(x)
        
        # ç¬¬äºŒä¸ªå·ç§¯
        x = layers.Conv2D(n_filter, (3, 3), padding="same")(x)
        x = layers.BatchNormalization()(x)
        
        # æ®‹å·®è¿æ¥
        if residual.shape[-1] != x.shape[-1]:
            residual = layers.Conv2D(n_filter, (1, 1), padding="same")(residual)
        x = layers.Add()([x, residual])
        x = layers.ReLU()(x)
        
        # é€‚åº¦é™é‡‡æ ·
        if i % 2 == 1:
            x = layers.MaxPooling2D((1, 2))(x)
    
    # å…¨å±€å¹³å‡æ± åŒ–
    x = layers.GlobalAveragePooling2D()(x)
    
    # å…¨è¿æ¥å±‚
    fc_sizes = [80, 60, 40, 30] if n_classes >= 5 else [60, 40, 30]
    
    for size in fc_sizes:
        x = layers.Dense(size, activation="relu", kernel_regularizer="l2")(x)
        x = layers.Dropout(dropout_rate)(x)
    
    # è¾“å‡ºå±‚
    output_layer = layers.Dense(n_classes)(x)
    
    model = models.Model(input_layer, output_layer, name=model_name)
    return model


def transform_to_multidim_format(data_x, dim):
    """
    å°†æ•°æ®è½¬æ¢ä¸º(N, 6, len, dim)æ ¼å¼
    
    Parameters:
    -----------
    data_x : np.ndarray
        è¾“å…¥æ•°æ®
    dim : int
        ç›®æ ‡ç»´åº¦
        
    Returns:
    --------
    np.ndarray
        è½¬æ¢åçš„æ•°æ® (N, 6, len, dim)
    """
    if dim == 1:
        # 1ç»´æ•°æ®ï¼šä½¿ç”¨Transform2D2TRï¼Œç„¶åæ‰©å±•ç»´åº¦
        if len(data_x.shape) == 2:
            # (N, len) -> é€šè¿‡Transform2D2TR -> (N, 6, len) -> (N, 6, len, 1)
            transformed = Transform2D2TR(data_x, rescale=True, times=3)
            return np.expand_dims(transformed, axis=-1)
        else:
            # å¦‚æœå·²ç»æ˜¯3ç»´
            return np.expand_dims(data_x, axis=-1)
    else:
        # å¤šç»´æ•°æ®ï¼šæ¨¡æ‹Ÿ6ç§å˜æ¢
        N, d, length = data_x.shape
        result = np.zeros((N, 6, length, dim))
        
        # 6ç§å˜æ¢ï¼š
        # 1. åŸå§‹æ•°æ®
        result[:, 0, :, :] = data_x.transpose(0, 2, 1)  # (N, d, len) -> (N, len, d)
        
        # 2. å¹³æ–¹æ•°æ®
        result[:, 1, :, :] = np.square(data_x).transpose(0, 2, 1)
        
        # 3. ç«‹æ–¹æ•°æ®
        result[:, 2, :, :] = np.power(data_x, 3).transpose(0, 2, 1)
        
        # 4. ç§»åŠ¨å¹³å‡ (çª—å£=3)
        for i in range(N):
            for j in range(d):
                smoothed = np.convolve(data_x[i, j], np.ones(3)/3, mode='same')
                result[i, 3, :, j] = smoothed
        
        # 5. å·®åˆ†æ•°æ®
        diff_data = np.diff(data_x, axis=2, prepend=data_x[:, :, :1])
        result[:, 4, :, :] = diff_data.transpose(0, 2, 1)
        
        # 6. ç´¯ç§¯å’Œæ•°æ®
        cumsum_data = np.cumsum(data_x, axis=2)
        result[:, 5, :, :] = cumsum_data.transpose(0, 2, 1)
        
        return result


def generate_experiment_data(samples_per_class, length, dim, classes, preset='basic'):
    """
    ç”Ÿæˆå®éªŒæ•°æ®
    
    Parameters:
    -----------
    samples_per_class : int
        æ¯ç±»æ ·æœ¬æ•°
    length : int
        æ—¶é—´åºåˆ—é•¿åº¦
    dim : int
        æ•°æ®ç»´åº¦
    classes : int
        åˆ†ç±»æ•°é‡
    preset : str
        é¢„è®¾é…ç½®
        
    Returns:
    --------
    tuple
        (data_x, data_y, class_names)
    """
    print(f"\nç”Ÿæˆæ•°æ®: {classes}åˆ†ç±», {dim}ç»´, é•¿åº¦{length}, æ¯ç±»{samples_per_class}æ ·æœ¬")
    
    if dim == 1:
        # 1ç»´æ•°æ®ï¼šä½¿ç”¨åŸç‰ˆDataSetGen
        mean_arg = np.array([0.7, 5, -5, 1.2, 0.6])
        var_arg = np.array([0, 0.7, 0.3, 0.4, 0.2])
        slope_arg = np.array([0.5, 0.025, -0.025, 0.03, 0.015])
        
        dataset = DataSetGen(samples_per_class, length, mean_arg, var_arg, slope_arg, n_trim=20)
        data_x = dataset["data_x"]
        
        # é€‰æ‹©æŒ‡å®šæ•°é‡çš„ç±»åˆ«
        if classes == 3:
            # åˆ é™¤å‰ä¸¤ç±»ï¼Œä½¿ç”¨å3ç±»
            data_x = np.delete(data_x, np.arange(0, 2 * samples_per_class), 0)
            labels = [0, 1, 2]
            class_names = ['Variance Change', 'No Slope Change', 'Slope Change']
        elif classes == 5:
            labels = [0, 1, 2, 3, 4]
            class_names = ['No Mean Change', 'Mean Change', 'Variance Change', 'No Slope Change', 'Slope Change']
        else:
            # ä½¿ç”¨å‰classesç±»
            data_x = data_x[:classes * samples_per_class]
            labels = list(range(classes))
            all_names = ['No Mean Change', 'Mean Change', 'Variance Change', 'No Slope Change', 'Slope Change']
            class_names = all_names[:classes]
        
        data_y = np.repeat(labels, samples_per_class)
        
    else:
        # å¤šç»´æ•°æ®ï¼šä½¿ç”¨HighDimDataSetGen
        config = get_preset_config(preset, d=dim)
        
        # æ ¹æ®classesè°ƒæ•´é…ç½®
        if classes == 3:
            # åªä½¿ç”¨3ç§å˜ç‚¹ç±»å‹
            config['correlation_changes']['enabled'] = False  # å…³é—­ç›¸å…³æ€§å˜åŒ–
            config['structural_changes']['enabled'] = False  # å…³é—­ç»“æ„å˜åŒ–
            class_names = ['No Change', 'Mean Change', 'Variance Change']
        elif classes == 5:
            # ä½¿ç”¨æ‰€æœ‰5ç§ç±»å‹
            class_names = ['No Change', 'Mean Change', 'Variance Change', 'Correlation Change', 'Trend Change']
        
        # ç”Ÿæˆæ•°æ®
        dataset_dict = HighDimDataSetGen(
            N_sub=samples_per_class,
            n=length,  # ç›´æ¥ä½¿ç”¨ç›®æ ‡é•¿åº¦
            d=dim,
            mean_changes=config['mean_changes'],
            var_changes=config['var_changes'],
            correlation_changes=config['correlation_changes'],
            trend_changes=config['trend_changes'],
            structural_changes=config['structural_changes'],
            n_trim=0,  # ä¸è¿›è¡Œtrim
            noise_std=1.0,
            seed=2022
        )
        
        data_x = dataset_dict['data_x']
        data_y = np.array(dataset_dict['labels'])
        class_names = ['No Change'] + [name.replace('_', ' ').title() for name in dataset_dict['change_types'][1:]]
    
    return data_x, data_y, class_names


def run_experiment(classes, dim, samples_per_class=800, length=400, epochs=80, batch_size=64):
    """
    è¿è¡Œå•ä¸ªå®éªŒ
    
    Parameters:
    -----------
    classes : int
        åˆ†ç±»æ•°é‡
    dim : int
        æ•°æ®ç»´åº¦
    samples_per_class : int
        æ¯ç±»æ ·æœ¬æ•°
    length : int
        æ—¶é—´åºåˆ—é•¿åº¦
    epochs : int
        è®­ç»ƒè½®æ•°
    batch_size : int
        æ‰¹æ¬¡å¤§å°
        
    Returns:
    --------
    dict
        å®éªŒç»“æœ
    """
    print(f"\n{'='*60}")
    print(f"å®éªŒ: {classes}åˆ†ç±», {dim}ç»´åº¦")
    print(f"{'='*60}")
    
    # ç”Ÿæˆæ•°æ®
    data_x, data_y, class_names = generate_experiment_data(
        samples_per_class, length, dim, classes
    )
    
    # è½¬æ¢ä¸ºå¤šç»´æ ¼å¼
    data_x_transformed = transform_to_multidim_format(data_x, dim)
    print(f"è½¬æ¢åæ•°æ®å½¢çŠ¶: {data_x_transformed.shape}")
    
    # æ•°æ®åˆ†å‰²
    x_train, x_test, y_train, y_test = train_test_split(
        data_x_transformed, data_y, 
        train_size=0.8, 
        random_state=42,
        stratify=data_y
    )
    
    print(f"è®­ç»ƒé›†: {x_train.shape}, æµ‹è¯•é›†: {x_test.shape}")
    
    # åˆ›å»ºæ¨¡å‹
    input_shape = x_train.shape[1:]  # (6, length, dim)
    model_name = f"deep_multidim_{classes}c_{dim}d"
    
    # æ ¹æ®å¤æ‚åº¦è°ƒæ•´å‚æ•°
    n_filter = 16 if dim <= 5 else 24
    dropout_rate = 0.3 + (classes - 3) * 0.05 + (dim - 1) * 0.02
    dropout_rate = min(dropout_rate, 0.5)
    n_resblock = 3 if classes <= 3 else 4
    
    model = create_multidim_deep_network(
        input_shape=input_shape,
        n_classes=classes,
        n_filter=n_filter,
        dropout_rate=dropout_rate,
        n_resblock=n_resblock,
        model_name=model_name
    )
    
    print(f"\næ¨¡å‹å‚æ•°:")
    print(f"è¾“å…¥å½¢çŠ¶: {input_shape}")
    print(f"æ»¤æ³¢å™¨æ•°: {n_filter}")
    print(f"Dropoutç‡: {dropout_rate:.3f}")
    print(f"æ®‹å·®å—æ•°: {n_resblock}")
    
    model.summary()
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    logdir = Path("logs_multidim_deep", f"{model_name}_{timestamp}")
    logdir.mkdir(parents=True, exist_ok=True)
    
    # è®­ç»ƒæ¨¡å‹
    learning_rate = 8e-4
    epochdots = tfdoc_model.EpochDots()
    
    history = compile_and_fit(
        model,
        x_train,
        y_train,
        batch_size,
        learning_rate,
        model_name,
        logdir,
        epochdots,
        validation_split=0.25,
        max_epochs=epochs,
    )
    
    # è¯„ä¼°æ¨¡å‹
    eval_results = model.evaluate(x_test, y_test, verbose=0)
    if isinstance(eval_results, list):
        test_loss = eval_results[0]
        test_accuracy = eval_results[1] if len(eval_results) > 1 else None
    else:
        test_loss = eval_results
        test_accuracy = None
    
    y_pred = np.argmax(model.predict(x_test, verbose=0), axis=1)
    
    print(f"\næµ‹è¯•ç»“æœ:")
    if test_accuracy is not None:
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
    else:
        # è®¡ç®—å‡†ç¡®ç‡ä»é¢„æµ‹ç»“æœ
        test_accuracy = np.mean(y_pred == y_test)
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f} (æ‰‹åŠ¨è®¡ç®—)")
    print(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}")
    
    # ç”Ÿæˆæ··æ·†çŸ©é˜µ
    confusion_mtx = tf.math.confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        confusion_mtx,
        cmap="YlGnBu",
        xticklabels=class_names,
        yticklabels=class_names,
        annot=True,
        fmt="g",
    )
    plt.xlabel("Prediction")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix: {classes}åˆ†ç±», {dim}ç»´")
    
    cm_path = logdir / f"{model_name}_confusion_matrix.png"
    plt.savefig(cm_path)
    plt.close()
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    plotter = tfdoc_plot.HistoryPlotter(metric="accuracy", smoothing_std=10)
    plt.figure(figsize=(10, 6))
    plotter.plot({model_name: history})
    plt.title(f"Training History: {classes}åˆ†ç±», {dim}ç»´")
    
    acc_path = logdir / f"{model_name}_training_history.png"
    plt.savefig(acc_path)
    plt.close()
    
    # ä¿å­˜æ¨¡å‹
    model_path = logdir / "model"
    model.save(model_path)
    
    # ä¿å­˜æ··æ·†çŸ©é˜µæ•°æ®
    np.save(logdir / "confusion_matrix.npy", confusion_mtx)
    
    return {
        'classes': classes,
        'dim': dim,
        'test_accuracy': test_accuracy,
        'test_loss': test_loss,
        'model_name': model_name,
        'logdir': str(logdir),
        'confusion_matrix': confusion_mtx.numpy()
    }


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰å®éªŒ"""
    print("å¤šç»´æ·±åº¦ç½‘ç»œå®éªŒ")
    print("è¾“å…¥æ ¼å¼: (batch_size, 6, len, dim)")
    print("å®éªŒè®¾ç½®: 3/5åˆ†ç±» Ã— 1/5/8ç»´")
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(2022)
    tf.random.set_seed(2022)
    
    # å®éªŒé…ç½®
    class_numbers = [3, 5]
    dimensions = [1, 5, 8]
    
    # å®éªŒå‚æ•°
    samples_per_class = 800
    length = 400
    epochs = 80
    batch_size = 64
    
    results = []
    
    print(f"\nå®éªŒå‚æ•°:")
    print(f"æ¯ç±»æ ·æœ¬æ•°: {samples_per_class}")
    print(f"æ—¶é—´åºåˆ—é•¿åº¦: {length}")
    print(f"è®­ç»ƒè½®æ•°: {epochs}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    total_experiments = len(class_numbers) * len(dimensions)
    current_exp = 0
    
    for classes in class_numbers:
        for dim in dimensions:
            current_exp += 1
            print(f"\nè¿›åº¦: {current_exp}/{total_experiments}")
            
            try:
                result = run_experiment(
                    classes=classes,
                    dim=dim,
                    samples_per_class=samples_per_class,
                    length=length,
                    epochs=epochs,
                    batch_size=batch_size
                )
                results.append(result)
                
                print(f"âœ“ å®éªŒå®Œæˆ: {classes}åˆ†ç±», {dim}ç»´ - å‡†ç¡®ç‡: {result['test_accuracy']:.4f}")
                
            except Exception as e:
                print(f"âœ— å®éªŒå¤±è´¥: {classes}åˆ†ç±», {dim}ç»´ - é”™è¯¯: {str(e)}")
                import traceback
                traceback.print_exc()
    
    # ä¿å­˜å’Œå±•ç¤ºç»“æœ
    if results:
        results_df = pd.DataFrame(results)
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_path = Path("logs_multidim_deep") / f"experiment_results_{timestamp}.csv"
        results_df.to_csv(results_path, index=False)
        
        print(f"\n{'='*60}")
        print("å®éªŒç»“æœæ±‡æ€»")
        print(f"{'='*60}")
        
        print("\nå‡†ç¡®ç‡ç»“æœ:")
        pivot_table = results_df.pivot(index='dim', columns='classes', values='test_accuracy')
        print(pivot_table)
        
        # å¯è§†åŒ–ç»“æœ
        plt.figure(figsize=(12, 8))
        
        # å‡†ç¡®ç‡çƒ­åŠ›å›¾
        plt.subplot(2, 2, 1)
        sns.heatmap(pivot_table, annot=True, fmt='.4f', cmap='YlOrRd', 
                   cbar_kws={'label': 'Test Accuracy'})
        plt.title('æµ‹è¯•å‡†ç¡®ç‡')
        plt.xlabel('åˆ†ç±»æ•°')
        plt.ylabel('ç»´åº¦')
        
        # å‡†ç¡®ç‡æ¡å½¢å›¾
        plt.subplot(2, 2, 2)
        results_df['exp_label'] = results_df['classes'].astype(str) + 'c_' + results_df['dim'].astype(str) + 'd'
        sorted_results = results_df.sort_values('test_accuracy', ascending=True)
        
        bars = plt.barh(range(len(sorted_results)), sorted_results['test_accuracy'])
        plt.yticks(range(len(sorted_results)), sorted_results['exp_label'])
        plt.xlabel('æµ‹è¯•å‡†ç¡®ç‡')
        plt.title('å‡†ç¡®ç‡æ’è¡Œ')
        plt.grid(axis='x', alpha=0.3)
        
        # æŒ‰åˆ†ç±»æ•°åˆ†ç»„
        plt.subplot(2, 2, 3)
        class_acc = results_df.groupby('classes')['test_accuracy'].agg(['mean', 'std']).reset_index()
        plt.bar(class_acc['classes'], class_acc['mean'], yerr=class_acc['std'], capsize=5)
        plt.xlabel('åˆ†ç±»æ•°')
        plt.ylabel('å¹³å‡å‡†ç¡®ç‡')
        plt.title('æŒ‰åˆ†ç±»æ•°åˆ†ç»„çš„å‡†ç¡®ç‡')
        plt.grid(axis='y', alpha=0.3)
        
        # æŒ‰ç»´åº¦åˆ†ç»„
        plt.subplot(2, 2, 4)
        dim_acc = results_df.groupby('dim')['test_accuracy'].agg(['mean', 'std']).reset_index()
        plt.bar(dim_acc['dim'], dim_acc['mean'], yerr=dim_acc['std'], capsize=5)
        plt.xlabel('ç»´åº¦')
        plt.ylabel('å¹³å‡å‡†ç¡®ç‡')
        plt.title('æŒ‰ç»´åº¦åˆ†ç»„çš„å‡†ç¡®ç‡')
        plt.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        
        summary_path = Path("logs_multidim_deep") / f"experiment_summary_{timestamp}.png"
        plt.savefig(summary_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"\nç»“æœå·²ä¿å­˜:")
        print(f"è¯¦ç»†ç»“æœ: {results_path}")
        print(f"æ±‡æ€»å›¾è¡¨: {summary_path}")
        
        # æ˜¾ç¤ºæœ€ä½³ç»“æœ
        best_result = results_df.loc[results_df['test_accuracy'].idxmax()]
        print(f"\nğŸ† æœ€ä½³ç»“æœ:")
        print(f"é…ç½®: {best_result['classes']}åˆ†ç±», {best_result['dim']}ç»´")
        print(f"å‡†ç¡®ç‡: {best_result['test_accuracy']:.4f}")
        print(f"æ¨¡å‹: {best_result['model_name']}")


if __name__ == "__main__":
    main() 